{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon', 'is', 'testing', 'drones', 'under', 'its', 'Prime', 'Air', 'brand', 'and', 'has', 'stated', 'that', '86', '%', 'of', 'their', 'packages', 'weigh', 'less', 'than', 'five', 'pounds', '.', 'UPS', 'is', 'testing', 'drones', 'that', 'could', 'launch', 'from', 'a', 'traditional', 'delivery', 'vehicle', ',', 'allowing', 'drivers', 'to', 'deliver', 'more', 'packages', 'and', 'save', 'on', 'fuel', 'costs', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "text = \"Amazon is testing drones under its Prime Air brand and has stated that 86% of their packages weigh less than five pounds. UPS is testing drones that could launch from a traditional delivery vehicle, allowing drivers to deliver more packages and save on fuel costs.\"\n",
    "print(wt(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon is testing drones under its Prime Air brand and has stated that 86% of their packages weigh less than five pounds.', 'UPS is testing drones that could launch from a traditional delivery vehicle, allowing drivers to deliver more packages and save on fuel costs.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['Amazon', 'is', 'testing', 'drones', 'under', 'its', 'Prime', 'Air', 'brand', 'and', 'has', 'stated', 'that', '86', '%', 'of', 'their', 'packages', 'weigh', 'less', 'than', 'five', 'pounds', '.', 'UPS', 'is', 'testing', 'drones', 'that', 'could', 'launch', 'from', 'a', 'traditional', 'delivery', 'vehicle', ',', 'allowing', 'drivers', 'to', 'deliver', 'more', 'packages', 'and', 'save', 'on', 'fuel', 'costs', '.']\n",
      "['Amazon', 'testing', 'drones', 'Prime', 'Air', 'brand', 'stated', '86', '%', 'packages', 'weigh', 'less', 'five', 'pounds', '.', 'UPS', 'testing', 'drones', 'could', 'launch', 'traditional', 'delivery', 'vehicle', ',', 'allowing', 'drivers', 'deliver', 'packages', 'save', 'fuel', 'costs', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "\n",
    "print(text_tokens)\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon is test drone under it prime air brand and ha state that 86 % of their packag weigh less than five pound . up is test drone that could launch from a tradit deliveri vehicl , allow driver to deliv more packag and save on fuel cost .\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "wordList = nltk.word_tokenize(text)\n",
    "\n",
    "stemWords = [porterStemmer.stem(word) for word in wordList]\n",
    "\n",
    "print(' '.join(stemWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Amazon              Amazon              \n",
      "is                  is                  \n",
      "testing             testing             \n",
      "drones              drone               \n",
      "under               under               \n",
      "its                 it                  \n",
      "Prime               Prime               \n",
      "Air                 Air                 \n",
      "brand               brand               \n",
      "and                 and                 \n",
      "has                 ha                  \n",
      "stated              stated              \n",
      "that                that                \n",
      "86                  86                  \n",
      "%                   %                   \n",
      "of                  of                  \n",
      "their               their               \n",
      "packages            package             \n",
      "weigh               weigh               \n",
      "less                le                  \n",
      "than                than                \n",
      "five                five                \n",
      "pounds              pound               \n",
      "UPS                 UPS                 \n",
      "is                  is                  \n",
      "testing             testing             \n",
      "drones              drone               \n",
      "that                that                \n",
      "could               could               \n",
      "launch              launch              \n",
      "from                from                \n",
      "a                   a                   \n",
      "traditional         traditional         \n",
      "delivery            delivery            \n",
      "vehicle             vehicle             \n",
      "allowing            allowing            \n",
      "drivers             driver              \n",
      "to                  to                  \n",
      "deliver             deliver             \n",
      "more                more                \n",
      "packages            package             \n",
      "and                 and                 \n",
      "save                save                \n",
      "on                  on                  \n",
      "fuel                fuel                \n",
      "costs               cost                \n"
     ]
    }
   ],
   "source": [
    "#word lemma\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "sentence_words = nltk.word_tokenize(text)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon',\n",
       " 'be',\n",
       " 'test',\n",
       " 'drone',\n",
       " 'under',\n",
       " 'it',\n",
       " 'Prime',\n",
       " 'Air',\n",
       " 'brand',\n",
       " 'and',\n",
       " 'have',\n",
       " 'state',\n",
       " 'that',\n",
       " '86',\n",
       " '%',\n",
       " 'of',\n",
       " 'their',\n",
       " 'package',\n",
       " 'weigh',\n",
       " 'le',\n",
       " 'than',\n",
       " 'five',\n",
       " 'pound',\n",
       " '.',\n",
       " 'UPS',\n",
       " 'be',\n",
       " 'test',\n",
       " 'drone',\n",
       " 'that',\n",
       " 'could',\n",
       " 'launch',\n",
       " 'from',\n",
       " 'a',\n",
       " 'traditional',\n",
       " 'delivery',\n",
       " 'vehicle',\n",
       " ',',\n",
       " 'allow',\n",
       " 'driver',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'more',\n",
       " 'package',\n",
       " 'and',\n",
       " 'save',\n",
       " 'on',\n",
       " 'fuel',\n",
       " 'cost',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon', 'is', 'testing', 'drones', 'under', 'its', 'Prime', 'Air', 'brand', 'and', 'has', 'stated', 'that', '86%', 'of', 'their', 'packages', 'weigh', 'less', 'than', 'five', 'pounds.', 'UPS', 'is', 'testing', 'drones', 'that', 'could', 'launch', 'from', 'a', 'traditional', 'delivery', 'vehicle,', 'allowing', 'drivers', 'to', 'deliver', 'more', 'packages', 'and', 'save', 'on', 'fuel', 'costs.']\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# using Whitespace Tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokens=WhitespaceTokenizer()\n",
    "output=tokens.tokenize(text)\n",
    "print(output)\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon', 'is', 'testing', 'drones', 'under', 'its', 'Prime', 'Air', 'brand', 'and', 'has', 'stated', 'that', '86', '%', 'of', 'their', 'packages', 'weigh', 'less', 'than', 'five', 'pounds', '.', 'UPS', 'is', 'testing', 'drones', 'that', 'could', 'launch', 'from', 'a', 'traditional', 'delivery', 'vehicle', ',', 'allowing', 'drivers', 'to', 'deliver', 'more', 'packages', 'and', 'save', 'on', 'fuel', 'costs', '.']\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# using WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokens=WordPunctTokenizer()\n",
    "output=tokens.tokenize(text)\n",
    "print(output)\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon', 'is', 'testing', 'drones', 'under', 'its', 'Prime', 'Air', 'brand', 'and', 'has', 'stated', 'that', '86', '%', 'of', 'their', 'packages', 'weigh', 'less', 'than', 'five', 'pounds.', 'UPS', 'is', 'testing', 'drones', 'that', 'could', 'launch', 'from', 'a', 'traditional', 'delivery', 'vehicle', ',', 'allowing', 'drivers', 'to', 'deliver', 'more', 'packages', 'and', 'save', 'on', 'fuel', 'costs', '.']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# using TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokens=TreebankWordTokenizer()\n",
    "output=tokens.tokenize(text)\n",
    "print(output)\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
